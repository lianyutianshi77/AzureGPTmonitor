import atexit
import streamlit as st
import pandas as pd
from datetime import datetime  
import pytz 
from tzlocal import get_localzone  
from apscheduler.schedulers.background import BackgroundScheduler  
from apscheduler.triggers.cron import CronTrigger  
from pathlib import Path
from croniter import croniter
from io import StringIO
from utils.data import GPTdata
from utils.gpt_request import process_image_resource, process_text_resource

def get_image_data():
    select_sql = """SELECT   
        region as 'åŒºåŸŸ',
        CASE
          WHEN model_name == 'gpt-4o' THEN deployment_name || ' ( ' ||  model_version || ' )' 
          ELSE model_name || ' ( ' ||  model_version || ' )' 
        END as 'æ¨¡å‹',
        MAX(update_time) AS 'æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰' ,
        status as 'çŠ¶æ€',
        CAST(ROUND(AVG(input_tokens), 0) AS INTEGER) AS 'è¾“å…¥Tokens',
        input_content_length as 'è¾“å…¥å†…å®¹é•¿åº¦',
        CAST(ROUND(AVG(output_tokens), 0) AS INTEGER) AS 'è¾“å‡ºTokens',
        output_content_length as 'è¾“å‡ºå†…å®¹é•¿åº¦',
        ROUND(AVG(total_time), 4) AS 'æ€»æ—¶é—´',
        ROUND(AVG(pretransfer_time), 4) AS 'é¦–Tokenæ—¶é—´',
        ROUND(AVG(namelookup_time), 4) AS 'DNSè§£ææ—¶é—´',
        ROUND(AVG(connect_time), 4) AS 'TCPå»ºç«‹æ—¶é—´'
    FROM   
        gpt_latency_data  
    WHERE type = 'IMAGE'
    GROUP BY   
        region,  
        resource_name,  
        deployment_name,  
        model_name,  
        model_version,  
        type
    ORDER BY deployment_name; 
    """
    db = GPTdata()
    data = db.query(select_sql)
    return data

def get_text_data():
    select_sql = """SELECT   
        region as 'åŒºåŸŸ',
        CASE
          WHEN model_name == 'gpt-4o' THEN deployment_name || ' ( ' ||  model_version || ' )' 
          ELSE model_name || ' ( ' ||  model_version || ' )' 
        END as 'æ¨¡å‹',
        MAX(update_time) AS 'æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰' ,
        status as 'çŠ¶æ€',
        CAST(ROUND(AVG(input_tokens), 0) AS INTEGER) AS 'è¾“å…¥Tokens',
        input_content_length as 'è¾“å…¥å†…å®¹é•¿åº¦',
        CAST(ROUND(AVG(output_tokens), 0) AS INTEGER) AS 'è¾“å‡ºTokens',
        output_content_length as 'è¾“å‡ºå†…å®¹é•¿åº¦',
        ROUND(AVG(total_time), 4) AS 'æ€»æ—¶é—´',
        ROUND(AVG(pretransfer_time), 4) AS 'é¦–Tokenæ—¶é—´',
        ROUND(AVG(namelookup_time), 4) AS 'DNSè§£ææ—¶é—´',
        ROUND(AVG(connect_time), 4) AS 'TCPå»ºç«‹æ—¶é—´'
    FROM   
        gpt_latency_data  
    WHERE type = 'TEXT'
    GROUP BY   
        region,  
        resource_name,  
        deployment_name,  
        model_name,  
        model_version,  
        type
    ORDER BY deployment_name; 
    """
    db = GPTdata()
    data = db.query(select_sql)
    return data

def convert_to_utc_plus_8(utc_time_str):
    time_format = "%Y-%m-%d %H:%M:%S"  
    utc_time = datetime.strptime(utc_time_str, time_format)      
    utc_time = utc_time.replace(tzinfo=pytz.utc)  
    utc_plus_8_time = utc_time.astimezone(pytz.timezone("Asia/Shanghai"))    
    return utc_plus_8_time.strftime(time_format) 

st.set_page_config(
    page_title="Azure GPT Latency",  # ç½‘é¡µæ ‡é¢˜
    page_icon="ğŸª",                         # ç½‘é¡µå›¾æ ‡ï¼Œå¯ä»¥æ˜¯ emoji
    layout="wide"                           # ç½‘é¡µå¸ƒå±€ï¼Œä½¿ç”¨ "wide" ä½¿å†…å®¹å æ®æ•´ä¸ªå®½åº¦
)

st.title("Azure GPT å„åŒºæ€§èƒ½ (ä»…ä¾›å‚è€ƒ)")
# æœ¬åœ°æ—¶åŒº
local_timezone = get_localzone()
local_datetime = datetime.now(local_timezone)
local_time_str = local_datetime.strftime("%Y-%m-%d %H:%M:%S")  
st.write(f'å®¢æˆ·ç«¯æ—¶é—´: {local_datetime.strftime("%Y-%m-%d %H:%M:%S %Z%z")} ( {local_timezone} ), åŒ—äº¬æ—¶é—´ï¼š{convert_to_utc_plus_8(local_time_str)} ( Asia/Shanghai )')
# st.write("æ¯ä¸¤æ—¶æ‰§è¡Œä¸€æ¬¡, å‘èµ·ç«¯åœ¨éŸ©å›½ï¼Œ")
with st.expander(f"æ¯ä¸¤æ—¶æ‰§è¡Œä¸€æ¬¡, å‘èµ·ç«¯åœ¨éŸ©å›½ï¼Œ æŸ¥çœ‹å…¶ä»–è¯´æ˜"):
    st.text("""
    1. æ¯ä¸¤å°æ—¶å‘èµ·ä¸€æ¬¡å“åº”æ—¶é—´æµ‹è¯•ï¼Œæ¯æ¬¡è¯·æ±‚è¿”å›é™åˆ¶åœ¨60ä¸ªtokensï¼Œæ¯æ¬¡è¿ç»­å‘é€ä¸‰ä¸ªè¯·æ±‚ï¼Œç»“æœå–å¹³å‡å€¼
    2. å½“å‰ä¸ä¿ç•™å†å²æµ‹è¯•ç»“æœï¼Œåªä¿ç•™æœ€è¿‘ä¸€æ¬¡è¯·æ±‚ç»“æœã€‚
    3. çŠ¶æ€ï¼šâœ…è¡¨ç¤ºæ­£å¸¸ï¼ŒâŒè¡¨ç¤ºå¼‚å¸¸ï¼ˆå¯èƒ½è®¤è¯å¤±è´¥/è¶…æ—¶/è§¦å‘æ¥å£é™åˆ¶ç­‰ï¼‰ï¼›
""")
# st.markdown("---")
st.subheader("å›¾ç‰‡ä»»åŠ¡ï¼š")
img_data = get_image_data()
if img_data is None or len(img_data) == 0:
    st.write("æš‚æ— æ•°æ®")
else:
    img_df = pd.DataFrame(img_data)
    # st.table(df)  
    height = len(img_df) * 37
    img_max = img_df["æ€»æ—¶é—´"].max()
    img_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"] = img_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"].apply(convert_to_utc_plus_8)
    img_df['çŠ¶æ€'] = img_df['çŠ¶æ€'].apply(lambda x: f'âœ…{x}' if x == 200 else f'âŒ{x}')  
    # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
    # st.dataframe(styled_df, hide_index=True)
    st.dataframe(
        img_df,
        column_config={
            "æ€»æ—¶é—´": st.column_config.ProgressColumn(
                "æ€»æ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=img_max,
            ),
            "é¦–Tokenæ—¶é—´": st.column_config.ProgressColumn(
                "é¦–Tokenæ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=img_max,
            ),
        },
        height=height,
        hide_index=True,
    )
st.markdown("---")
st.subheader("æ–‡æœ¬ä»»åŠ¡ï¼š")
text_data = get_text_data()
if text_data is None or len(text_data) == 0:
    st.write("æš‚æ— æ•°æ®")
else:
    text_df = pd.DataFrame(text_data)
    # st.table(df)  
    text_height = len(text_df) * 38
    text_max = text_df["æ€»æ—¶é—´"].max()
    text_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"] = text_df["æ›´æ–°æ—¶é—´ï¼ˆ+8ï¼‰"].apply(convert_to_utc_plus_8)
    text_df['çŠ¶æ€'] = text_df['çŠ¶æ€'].apply(lambda x: f'âœ…{x}' if x == 200 else f'âŒ{x}')  
    # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
    # st.dataframe(styled_df, hide_index=True)
    st.dataframe(
        text_df,
        column_config={
            "æ€»æ—¶é—´": st.column_config.ProgressColumn(
                "æ€»æ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=text_max,
            ),
            "é¦–Tokenæ—¶é—´": st.column_config.ProgressColumn(
                "é¦–Tokenæ—¶é—´",
                help="å–ä¸‰æ¬¡è¯·æ±‚çš„å¹³å‡å€¼",
                format="%f",
                min_value=0,
                max_value=text_max,
            ),
        },
        height=text_height,
        hide_index=True,
    )

st.markdown("---")
st.write("Support by Min")
  
# åˆå§‹åŒ–è°ƒåº¦å™¨  
scheduler = BackgroundScheduler()  
cron_expression = "10 */2 * * *"  # crontab è¡¨è¾¾å¼ï¼Œ æ¯ä¸¤å°æ—¶æ‰§è¡Œä¸€æ¬¡
scheduler_lock = "scheduler.lock"

def update_next_time():
    cron = croniter(cron_expression, datetime.now())
    next_time = cron.get_next(datetime).strftime("%Y-%m-%d %H:%M:%S")
    st.session_state['next_time'] = next_time
    print(f"ä¸‹æ¬¡ä»»åŠ¡æ‰§è¡Œæ—¶é—´: {next_time}")
    
    if Path(scheduler_lock).exists():
        df = pd.read_json(scheduler_lock, orient='records')
    else:
        df = pd.DataFrame(columns=["job", "next_time"])
    
    df["next_time"] = next_time
    df.to_json(scheduler_lock, orient='records')
  
def run_task(): 
    print(f"å®šæ—¶ä»»åŠ¡è¿è¡Œä¸­... {datetime.now()}")
    process_image_resource()  
    process_text_resource()  
    print(f"å®šæ—¶ä»»åŠ¡ç»“æŸ... {datetime.now()}")  
    update_next_time()

def start_scheduler():  
    if 'job' not in st.session_state:  
        try:  
            update_next_time()
            trigger = CronTrigger.from_crontab(cron_expression)  
            job = scheduler.add_job(run_task, trigger)
            st.session_state['job'] = job 

            df = pd.DataFrame([{"job":  str(st.session_state['job']), "next_time": st.session_state['next_time']}])  
            df.to_json(scheduler_lock, orient='records')
            
            scheduler.start()  
            print(f"ä»»åŠ¡å¯åŠ¨æˆåŠŸ, å¾…æ‰§è¡Œæ—¶é—´: {st.session_state['next_time']}")  
        except Exception as e:  
            print(f"å¯åŠ¨ä»»åŠ¡æ—¶å‡ºé”™: {e}")  
    else:  
        print(f"ä»»åŠ¡ä¿¡æ¯ï¼š{st.session_state['job']}")  
        print("å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œä¸­!")  
  
def load_existing_scheduler():  
    if Path(scheduler_lock).exists():  
        df = pd.read_json(scheduler_lock, orient='records')  
        print(f"å·²æœ‰è°ƒåº¦å™¨åœ¨è¿è¡Œä¸­! {df.to_dict()}")  
    else:  
        start_scheduler()  
  
def cleanup():  
    print("åº”ç”¨é€€å‡ºï¼Œæ¸…ç†èµ„æº...")  
    if scheduler.running:  
        scheduler.shutdown()  
  
atexit.register(cleanup)  
  
# åˆå§‹åŒ–  
load_existing_scheduler()  
